[{"authors":["admin"],"categories":null,"content":"I graduated from Tsinghua University with a doctor\u0026rsquo;s degree. My research covers reinforcement learning, autonomous driving, and optimal control. In Tsinghua, I worked at Intelligent Driving Lab (iDLab), where I was fortunate to be advised by Shengbo Eben Li and Bo Cheng. Since my graduate study, I have been committed to building more efficient and safer driving AI for the decision-making and control of automated vehicles, and to developing the next generation reinforcement learning algorithms that fuse both the data and the human knowledge for its application on advanced autonomous driving techniques.\nCurrently, I am working in Meituan autonomous driving team. Together with my talented colleagues, I am devoted to developing the most advanced technologies for the decision-making intelligence of automated vehicles, to build a safer, smarter delivery robot. And with it, make service available in every corner of the world!\n","date":1666051200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1666051200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yangguan.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I graduated from Tsinghua University with a doctor\u0026rsquo;s degree. My research covers reinforcement learning, autonomous driving, and optimal control. In Tsinghua, I worked at Intelligent Driving Lab (iDLab), where I was fortunate to be advised by Shengbo Eben Li and Bo Cheng. Since my graduate study, I have been committed to building more efficient and safer driving AI for the decision-making and control of automated vehicles, and to developing the next generation reinforcement learning algorithms that fuse both the data and the human knowledge for its application on advanced autonomous driving techniques.","tags":null,"title":"Yang Guan","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://yangguan.me/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Yang Guan"],"categories":null,"content":"","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://yangguan.me/publication/thesis/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"This paper studies a real-time and scalable decision and control method with safety assurance for automated vehicles. It also develops theory for mixed-driven reinforcement learning (RL) with high learning efficiency and performance, and builds a distributed asynchronous parallel computing toolchain for solving the driving policy. This work lays the foundation for the decision and control functionality of high-level automated vehicles.","tags":[],"title":"Integrated Decision and Control of Automated Vehicles and Its Training by Mixed Reinforcement Learning","type":"publication"},{"authors":["Jiaxin Gao*","Yang Guan*","Wenyu Li","Shengbo Eben Li","Fei Ma","Jianfeng Zheng","Junqing Wei","Bo Zhang","Keqiang Li"],"categories":null,"content":"(* indicates equal contribution.)\n","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"32012d60fe5ccd864460c159a9626045","permalink":"https://yangguan.me/publication/t3s/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/t3s/","section":"publication","summary":"Model-based policy gradient (MBPG) has been employed to seek an approximate solution to the optimal control problem. However, there is coupling between adjacent states due to temporal dependencies, making the training time grow linearly with the time horizon. This paper reshapes the training process of MBPG with the time-splitting technique to establish a time-independent algorithm called Training Through Time-Splitting (T3S). First, copy the coupled variables to obtain two independent variables. Meanwhile, an extra variable together with an equivalence constraint is introduced for problem consistency. Then, the transformed problem divides into subproblems with carefully derived loss functions. Subproblems own decoupled variables and shared policy networks, which means they can be optimized concurrently. Guided by the algorithm design, this paper further proposes an asynchronous parallel training scheme to accelerate training efficiency. Numerical simulation shows that the T3S algorithm outperforms the MBPG algorithm by 83.6% in wall-clock time with a trajectory tracking task.","tags":[],"title":"Beyond Backpropagate Through Time: Efficient Model-Based Training Through Time-Splitting","type":"publication"},{"authors":["Yang Guan","Yangang Ren","Qi Sun","Shengbo Eben Li","Haitong Ma","Jingliang Duan","Yifan Dai","Bo Cheng"],"categories":null,"content":"","date":1650326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650326400,"objectID":"c2cb757883a9a665d965c61aa2d1ef06","permalink":"https://yangguan.me/publication/integrated/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/integrated/","section":"publication","summary":"Decision and control are core functionalities of high-level automated vehicles. Current mainstream methods, such as functional decomposition and end-to-end reinforcement learning (RL), suffer high time complexity or poor interpretability and adaptability on real-world autonomous driving tasks. In this article, we present an interpretable and computationally efficient framework called integrated decision and control (IDC) for automated vehicles, which decomposes the driving task into static path planning and dynamic optimal tracking that are structured hierarchically. First, the static path planning generates several candidate paths only considering static traffic elements. Then, the dynamic optimal tracking is designed to track the optimal path while considering the dynamic obstacles. To that end, we formulate a constrained optimal control problem (OCP) for each candidate path, optimize them separately, and follow the one with the best tracking performance. To unload the heavy online computation, we propose a model-based RL algorithm that can be served as an approximate-constrained OCP solver. Specifically, the OCPs for all paths are considered together to construct a single complete RL problem and then solved offline in the form of value and policy networks for real-time online path selecting and tracking, respectively. We verify our framework in both simulations and the real world. Results show that compared with baseline methods, IDC has an order of magnitude higher online computing efficiency, as well as better driving performance, including traffic efficiency and safety. In addition, it yields great interpretability and adaptability among different driving scenarios and tasks.","tags":[],"title":"Integrated Decision and Control: Toward Interpretable and Efficient Driving Intelligence","type":"publication"},{"authors":["Jianhua Jiang","Yangang Ren","Yang Guan","Shengbo Eben Li","Yuming Yin","Dongjie Yu","Xiaoping Jin"],"categories":null,"content":"","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"dc70bce96aaa036cae6893c728803da5","permalink":"https://yangguan.me/publication/idc_mix/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/publication/idc_mix/","section":"publication","summary":"Autonomous driving at intersections is one of the most complicated and accident-prone traffic scenarios, especially with mixed traffic participants such as vehicles, bicycles and pedestrians. The driving policy should make safe decisions to handle the dynamic traffic conditions and meet the requirements of on-board computation. However, most of the current researches focuses on simplified intersections considering only the surrounding vehicles and idealized traffic lights. This paper improves the integrated decision and control framework and develops a learning-based algorithm to deal with complex intersections with mixed traffic flows, which can not only take account of realistic characteristics of traffic lights, but also learn a safe policy under different safety constraints. We first consider different velocity models for green and red lights in the training process and use a finite state machine to handle different modes of light transformation. Then we design different types of distance constraints for vehicles, traffic lights, pedestrians, bicycles respectively and formulize the constrained optimal control problems (OCPs) to be optimized. Finally, reinforcement learning (RL) with value and policy networks is adopted to solve the series of OCPs. In order to verify the safety and efficiency of the proposed method, we design a multi-lane intersection with the existence of large-scale mixed traffic participants and set practical traffic light phases. The simulation results indicate that the trained decision and control policy can well balance safety and tracking performance. Compared with model predictive control (MPC), the computational time is three orders of magnitude lower.","tags":[],"title":"Integrated Decision and Control at Multi-Lane Intersections with Mixed Traffic Flow","type":"publication"},{"authors":["Yang Guan*","Yangang Ren*","Haitong Ma","Shengbo Eben Li","Qi Sun","Yifan Dai","Bo Cheng"],"categories":null,"content":"(* indicates equal contribution.)\n","date":1632009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632009600,"objectID":"e0ba2c212dfd72262ed6c9914a8db25a","permalink":"https://yangguan.me/publication/idc_itsc/","publishdate":"2021-05-20T00:00:00Z","relpermalink":"/publication/idc_itsc/","section":"publication","summary":"Intersection is one of the most complex and accident-prone urban traffic scenarios for autonomous driving wherein making safe and computationally efficient decisions with high-density traffic flow is usually non-trivial. Current rule-based methods decompose the decision-making task into several serial sub-modules, resulting in long computation time at complex scenarios for on-board computing devices. In this paper, we formulate the decision-making and control problem under intersections as a process of optimal path selection and tracking, where the former selects a path with the best safety measure from a set generated only considering static information, while the latter then considers dynamic obstacles and solve a tracking problem with safety constraints using the chosen path. To avoid the heavy computation introduced by that, we develop a reinforcement learning algorithm called generalized exterior point (GEP) to find a neural network (NN) solution offline. It first constructs a multi-task problem involving all the candidate paths and transforms it into an unconstrained problem with a penalty on safety violations. Afterward, the approximate feasible optimal control policy is obtained by alternatively performing gradient descent and enlarging the penalty. As an exterior point type method, GEP permits control policy to violate inequality constraints during the iterations. To verify the effectiveness of our method, we carried out experiments both in simulation and in a real road test. Results demonstrate that the learned policy can realize collision-free driving under different traffic conditions while reducing the computation time by a large margin.","tags":[],"title":"Learn Collision-Free Self-Driving Skills at Urban Intersections with Model-Based Reinforcement Learning","type":"publication"},{"authors":["Baiyu Peng","Jingliang Duan","Jianyu Chen","Shengbo Eben Li","Genjin Xie","Congsheng Zhang","Yang Guan","Yao Mu","Enxin Sun"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"d089dcb29984e6ab62bddbeec7e9128c","permalink":"https://yangguan.me/publication/spil_journal/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/spil_journal/","section":"publication","summary":"Safety is essential for reinforcement learning (RL) applied in the real world. Adding chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under uncertainty. Existing chance-constrained RL methods like the penalty methods and the Lagrangian methods either exhibit periodic oscillations or learn an over-conservative or unsafe policy. In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. We first review the constrained policy optimization process from a feedback control perspective, which regards the penalty weight as the control input and the safe probability as the control output. Based on this, the penalty method is formulated as a proportional controller, and the Lagrangian method is formulated as an integral controller. We then unify them and present a proportional-integral Lagrangian method to get both their merits, with an integral separation technique to limit the integral value in a reasonable range. To accelerate training, the gradient of safe probability is computed in a model-based manner. We demonstrate our method can reduce the oscillations and conservatism of RL policy in a car-following simulation. To prove its practicality, we also apply our method to a real-world mobile robot navigation task, where our robot successfully avoids a moving obstacle with highly uncertain or even aggressive behaviors.","tags":[],"title":"Model-based Chance-Constrained Reinforcement Learning via Separated Proportional-Integral Lagrangian","type":"publication"},{"authors":["Baiyu Peng","Yao Mu","Jingliang Duan","Yang Guan","Shengbo Eben Li","Jianyu Chen"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"2c09672949bbf0cc6451ecced9c6fb20","permalink":"https://yangguan.me/publication/spil/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/spil/","section":"publication","summary":"Safety is essential for reinforcement learning (RL) applied in real-world tasks like autonomous driving. Imposing chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under model uncertainty. Existing chance constrained RL methods like the penalty methods and the Lagrangian methods either exhibit periodic oscillations or learn an over-conservative or unsafe policy. In this paper, we address these shortcomings by elegantly combining these two methods and propose a separated proportional-integral Lagrangian (SPIL) algorithm. We first rewrite penalty methods as optimizing safe probability according to the proportional value of constraint violation, and Lagrangian methods as optimizing according to the integral value of the violation. Then we propose to add up both the integral and proportion values to optimize the policy, with an integral separation technique to limit the integral value within a reasonable range. Besides, the gradient of policy is computed in a model-based paradigm to accelerate training. The proposed method is proved to reduce oscillations and conservatism while ensuring safety by a car-following experiment.","tags":[],"title":"Separated Proportional-Integral Lagrangian for Chance Constrained Reinforcement Learning","type":"publication"},{"authors":["Jingliang Duan*","Yang Guan*","Shengbo Eben Li","Yangang Ren","Qi Sun","Bo Cheng"],"categories":null,"content":"(* indicates equal contribution.)\n","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"8fd00e154310c37b9fabd529509c9706","permalink":"https://yangguan.me/publication/dsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/dsac/","section":"publication","summary":"In reinforcement learning (RL), function approximation errors are known to easily lead to the Q-value overestimations, thus greatly reducing policy performance. This article presents a distributional soft actor-critic (DSAC) algorithm, which is an off-policy RL method for continuous control setting, to improve the policy performance by mitigating Q-value overestimations. We first discover in theory that learning a distribution function of state-action returns can effectively mitigate Q-value overestimations because it is capable of adaptively adjusting the update step size of the Q-value function. Then, a distributional soft policy iteration (DSPI) framework is developed by embedding the return distribution function into maximum entropy RL. Finally, we present a deep off-policy actor-critic variant of DSPI, called DSAC, which directly learns a continuous return distribution by keeping the variance of the state-action returns within a reasonable range to address exploding and vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous control tasks, achieving the state-of-the-art performance.","tags":[],"title":"Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors","type":"publication"},{"authors":["Kaiming Tang","Shengbo Eben Li","Yuming Yin","Yang Guan","Jingliang Duan","Wenhan Cao","Jie Li"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"28edc765c7af387dfe13b07003c1a8b5","permalink":"https://yangguan.me/publication/rf/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/rf/","section":"publication","summary":"State estimation is critical to control systems, especially when the states cannot be directly measured. This paper presents an approximate optimal filter, which enables to use policy iteration technique to obtain the steady-state gain in linear Gaussian time-invariant systems. This design transforms the optimal filtering problem with minimum mean square error into an optimal control problem, called Approximate Optimal Filtering (AOF) problem. The equivalence holds given certain conditions about initial state distributions and policy formats, in which the system state is the estimation error, control input is the filter gain, and control objective function is the accumulated estimation error. We present a policy iteration algorithm to solve the AOF problem in steady-state. A classic vehicle state estimation problem finally evaluates the approximate filter. The results show that the policy converges to the steady-state Kalman gain, and its accuracy is within 2 %.","tags":[],"title":"Approximate Optimal Filter for Linear Gaussian Time-invariant Systems","type":"publication"},{"authors":["Yiting Kong*","Yang Guan*","Jingliang Duan","Shengbo Eben Li","Qi Sun","Bingbing Nie"],"categories":null,"content":"(* indicates equal contribution.)\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"5562fef4675674c463b9c3305f2e7324","permalink":"https://yangguan.me/publication/sdsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/sdsac/","section":"publication","summary":"Merging into the highway from the on-ramp is an essential scenario for automated driving. The decision-making under the scenario needs to balance the safety and efficiency performance to optimize a long-term objective, which is challenging due to the dynamic, stochastic, and adversarial characteristics. The existing rule-based methods often lead to conservative driving on this task while the learning-based methods have difficulties meeting the safety requirements. In this paper, we propose an reinforcement learning based end-to-end decision-making method under a framework of offline training and online correction, called the Shielded Distributional Soft Actor-critic (SDSAC). The SDSAC adopts the policy evaluation with safety consideration in offline training and a safety shield parameterized with the barrier function in online correction. These two measures support each other in achieving better safety without sacrificing the efficiency performance. We verify the SDSAC on an on-ramp merge scenario in simulation. The results show that the SDSAC has the best safety performance compared to baseline algorithms and achieves efficient driving simultaneously.","tags":[],"title":"Decision-Making under On-Ramp merge Scenarios by Distributional Soft Actor-Critic Algorithm","type":"publication"},{"authors":["Haitong Ma","Jianyu Chen","Shengbo Eben Li","Ziyu Lin","Yang Guan","Yangang Ren","Sifa Zheng"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"e7b4eefbc4a0e3d957d93525ad8516c8","permalink":"https://yangguan.me/publication/cfb/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/cfb/","section":"publication","summary":"Model information can be used to predict future trajectories, so it has huge potential to avoid dangerous region when implementing reinforcement learning (RL) on real-world tasks, like autonomous driving. However, existing studies mostly use model-free constrained RL, which causes inevitable constraint violations. This paper proposes a model-based feasibility enhancement technique of constrained RL, which enhances the feasibility of policy using generalized control barrier function (GCBF) defined on the distance to constraint boundary. By using the model information, the policy can be optimized safely without violating actual safety constraints, and the sample efficiency is increased. The major difficulty of infeasibility in solving the constrained policy gradient is handled by an adaptive coefficient mechanism. We evaluate the proposed method in both simulations and real vehicle experiments in a complex autonomous driving collision avoidance task. The proposed method achieves up to four times fewer constraint violations and converges 3.36 times faster than baseline constrained RL approaches.","tags":[],"title":"Model-based Constrained Reinforcement Learning using Generalized Control Barrier Function","type":"publication"},{"authors":["Yang Guan","Jingliang Duan","Shengbo Eben Li","Jie Li","Jianyu Chen","Bo Cheng"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"ec6e26e8fdd8936162c85ccc8e00e612","permalink":"https://yangguan.me/publication/mpg/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mpg/","section":"publication","summary":"Reinforcement learning (RL) has great potential in sequential decision-making. At present, the mainstream RL algorithms are data-driven, relying on millions of iterations and a large number of empirical data to learn a policy. Although data-driven RL may have excellent asymptotic performance, it usually yields slow convergence speed. As a comparison, model-driven RL employs a differentiable transition model to improve convergence speed, in which the policy gradient (PG) is calculated by using the backpropagation through time (BPTT) technique. However, such methods suffer from numerical instability, model error sensitivity and low computing efficiency, which may lead to poor policies. In this paper, a mixed policy gradient (MPG) method is proposed, which uses both empirical data and the transition model to construct the PG, so as to accelerate the convergence speed without losing the optimality guarantee. MPG contains two types of PG: 1) data-driven PG, which is obtained by directly calculating the derivative of the learned Q-value function with respect to actions, and 2) model-driven PG, which is calculated using BPTT based on the model-predictive return. We unify them by revealing the correlation between the upper bound of the unified PG error and the predictive horizon, where the data-driven PG is regraded as 0-step model-predictive return. Relying on that, MPG employs a rule-based method to adaptively adjust the weights of data-driven and model-driven PGs. In particular, to get a more accurate PG, the weight of the data-driven PG is designed to grow along the learning process while the other to decrease. Besides, an asynchronous learning framework is proposed to reduce the wall-clock time needed for each update iteration. Simulation results show that the MPG method achieves the best asymptotic performance and convergence speed compared with other baseline algorithms.","tags":[],"title":"Mixed Policy Gradient","type":"publication"},{"authors":["Yuhang Zhang","Yao Mu","Yujie Yang","Yang Guan","Shengbo Eben Li","Qi Sun","Jianyu Chen"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"2304010760d90a5f35ca2e0d4232853b","permalink":"https://yangguan.me/publication/lvm/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/lvm/","section":"publication","summary":"Reinforcement learning has shown great potential in developing high-level autonomous driving. However, for high-dimensional tasks, current RL methods suffer from low data efficiency and oscillation in the training process. This paper proposes an algorithm called Learn to drive with Virtual Memory (LVM) to overcome these problems. LVM compresses the high-dimensional information into compact latent states and learns a latent dynamic model to summarize the agent's experience. Various imagined latent trajectories are generated as virtual memory by the latent dynamic model. The policy is learned by propagating gradient through the learned latent model with the imagined latent trajectories and thus leads to high data efficiency. Furthermore, a double critic structure is designed to reduce the oscillation during the training process. The effectiveness of LVM is demonstrated by an image-input autonomous driving task, in which LVM outperforms the existing method in terms of data efficiency, learning stability, and control performance.","tags":[],"title":"Steadily Learn to Drive with Virtual Memory","type":"publication"},{"authors":["Baiyu Peng","Yao Mu","Yang Guan","Shengbo Eben Li","Yuming Yin","Jianyu Chen"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"69094b0462aaf8bd11c412463b909033","permalink":"https://yangguan.me/publication/ccac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/ccac/","section":"publication","summary":"Safety is essential for reinforcement learning (RL) applied in real-world situations. Chance constraints are suitable to represent the safety requirements in stochastic systems. Previous chance-constrained RL methods usually have a low convergence rate, or only learn a conservative policy. In this paper, we propose a model-based chance constrained actor-critic (CCAC) algorithm which can efficiently learn a safe and non-conservative policy. Different from existing methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems, where the objective function and safe probability is simultaneously optimized with adaptive weights. In order to improve the convergence rate, CCAC utilizes the gradient of dynamic model to accelerate policy optimization. The effectiveness of CCAC is demonstrated by a stochastic car-following task. Experiments indicate that compared with previous RL methods, CCAC improves the performance while guaranteeing safety, with a five times faster convergence rate. It also has 100 times higher online computation efficiency than traditional safety techniques such as stochastic model predictive control.","tags":[],"title":"Model-Based Actor-Critic with Chance Constraint for Stochastic System","type":"publication"},{"authors":["Yang Guan","Shengbo Eben Li","Jingliang Duan","Jie Li","Yangang Ren","Bo Cheng"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"3eb4d6fae1b99243ac7764f9627a19ab","permalink":"https://yangguan.me/publication/di_indi/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/di_indi/","section":"publication","summary":"Reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision-making and control tasks. In this paper, we classify RL into direct and indirect RL according to how they seek the optimal policy of the Markov decision process problem. The former solves the optimal policy by directly maximizing an objective function using gradient descent methods, in which the objective function is usually the expectation of accumulative future rewards. The latter indirectly finds the optimal policy by solving the Bellman equation, which is the sufficient and necessary condition from Bellman's principle of optimality. We study policy gradient (PG) forms of direct and indirect RL and show that both of them can derive the actor–critic architecture and can be unified into a PG with the approximate value function and the stationary state distribution, revealing the equivalence of direct and indirect RL. We employ a Gridworld task to verify the influence of different forms of PG, suggesting their differences and relationships experimentally. Finally, we classify current mainstream RL algorithms using the direct and indirect taxonomy, together with other ones, including value-based and policy-based, model-based and model-free.","tags":[],"title":"Direct and indirect reinforcement learning","type":"publication"},{"authors":null,"categories":null,"content":"The project aims to build an interpretable self-learning driving system by RL, for the real-time decision and control of automated vehicles. My works: 1) Formulated a general integrated decision and control framework, which utilizes RL as a way to solve constrained optimal control problems (OCP), and thus makes the output interpretable in the sense that it is the approximate solution of the OCP. The framework is promising to promote RL applications in real-world autonomous driving tasks. 2) Proposed a model-based RL algorithm for approximately solving large-scale constrained OCPs, where a generalized exterior point method is employed to find a feasible neural solution. 3) Carried out experiments both in simulation and in real world, yielding the best performance in terms of computing efficiency 10x and safety 100x compared with baseline methods.\n","date":1601164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601164800,"objectID":"27190523fd269ce0c7e4f86be54fa9b8","permalink":"https://yangguan.me/project/interpretable/","publishdate":"2020-09-27T00:00:00Z","relpermalink":"/project/interpretable/","section":"project","summary":"The project aims to build an interpretable self-learning driving system by RL, for the real-time decision and control of automated vehicles. My works: 1) Formulated a general integrated decision and control framework, which utilizes RL as a way to solve constrained optimal control problems (OCP), and thus makes the output interpretable in the sense that it is the approximate solution of the OCP. The framework is promising to promote RL applications in real-world autonomous driving tasks.","tags":["Reinforcement Learning","Autonomous Driving"],"title":"Interpretable Driving AI with Highly Efficient Online Computation and Self-evolution Ability","type":"project"},{"authors":["Yang Guan","Yangang Ren","Shengbo Eben Li","Qi Sun","Laiquan Luo","Keqiang Li"],"categories":null,"content":"","date":1600819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600819200,"objectID":"f1ab6bbcb93421d96b5ed2a7f8c53e0f","permalink":"https://yangguan.me/publication/mappo/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mappo/","section":"publication","summary":"Connected vehicles will change the modes of future transportation management and organization, especially at an intersection without traffic light. Centralized coordination methods globally coordinate vehicles approaching the intersection from all sections by considering their states altogether. However, they need substantial computation resources since they own a centralized controller to optimize the trajectories for all approaching vehicles in real-time. In this paper, we propose a centralized coordination scheme of automated vehicles at an intersection without traffic signals using reinforcement learning (RL) to address low computation efficiency suffered by current centralized coordination methods. We first propose an RL training algorithm, model accelerated proximal policy optimization (MA-PPO), which incorporates a prior model into proximal policy optimization (PPO) algorithm to accelerate the learning process in terms of sample efficiency. Then we present the design of state, action and reward to formulate centralized coordination as an RL problem. Finally, we train a coordinate policy in a simulation setting and compare computing time and traffic efficiency with a coordination scheme based on model predictive control (MPC) method. Results show that our method spends only 1/400 of the computing time of MPC and increase the efficiency of the intersection by 4.5 times.","tags":[],"title":"Centralized Cooperation for Connected and Automated Vehicles at Intersections by Proximal Policy Optimization","type":"publication"},{"authors":["Yangang Ren","Jingliang Duan","Shengbo Eben Li","Yang Guan","Qi Sun"],"categories":null,"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"333d33df1352da1dd71d42baa5c197ed","permalink":"https://yangguan.me/publication/minmax_dsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/minmax_dsac/","section":"publication","summary":"Reinforcement learning (RL) has achieved remarkable performance in numerous sequential decision making and control tasks. However, a common problem is that learned nearly optimal policy always overfits to the training environment and may not be extended to situations never encountered during training. For practical applications, the randomness of environment usually leads to some devastating events, which should be the focus of safety-critical systems such as autonomous driving. In this paper, we introduce the minimax formulation and distributional framework to improve the generalization ability of RL algorithms and develop the Minimax Distributional Soft Actor-Critic (Minimax DSAC) algorithm. Minimax formulation aims to seek optimal policy considering the most severe variations from environment, in which the protagonist policy maximizes action-value function while the adversary policy tries to minimize it. Distributional framework aims to learn a state-action return distribution, from which we can model the risk of different returns explicitly, thereby formulating a risk-averse protagonist policy and a risk-seeking adversarial policy. We implement our method on the decision-making tasks of autonomous vehicles at intersections and test the trained policy in distinct environments. Results demonstrate that our method can greatly improve the generalization ability of the protagonist agent to different environmental variations.","tags":[],"title":"Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic","type":"publication"},{"authors":["Jie Li","Shengbo Eben Li","Yang Guan","Jingliang Duan","Wenyu Li","Yuming Yin"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"cfad4e91f6bc841ae4e9c83ef45a357c","permalink":"https://yangguan.me/publication/ternary/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/ternary/","section":"publication","summary":"The uncertainties in plant dynamics remain a challenge for nonlinear control problems. This paper develops a ternary policy iteration (TPI) algorithm for solving nonlinear robust control problems with bounded uncertainties. The controller and uncertainty of the system are considered as game players, and the robust control problem is formulated as a two-player zero-sum differential game. In order to solve the differential game, the corresponding Hamilton-Jacobi-Isaacs (HJI) equation is then derived. Three loss functions and three update phases are designed to match the identity equation, minimization and maximization of the HJI equation, respectively. These loss functions are defined by the expectation of the approximate Hamiltonian in a generated state set to prevent operating all the states in the entire state set concurrently. The parameters of value function and policies are directly updated by diminishing the designed loss functions using the gradient descent method. Moreover, zero-initialization can be applied to the parameters of the control policy. The effectiveness of the proposed TPI algorithm is demonstrated through two simulation studies. The simulation results show that the TPI algorithm can converge to the optimal solution for the linear plant, and has high resistance to disturbances for the nonlinear plant.","tags":[],"title":"Ternary Policy Iteration Algorithm for Nonlinear Robust Control","type":"publication"},{"authors":["Jingliang Duan","Shengbo Eben Li","Yang Guan","Qi Sun","Bo Cheng"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"bf982a6c02259b4cb686094a43683388","permalink":"https://yangguan.me/publication/hier/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/hier/","section":"publication","summary":"Decision making for self-driving cars is usually tackled by manually encoding rules from drivers’ behaviours or imitating drivers’ manipulation using supervised learning techniques. Both of them rely on mass driving data to cover all possible driving scenarios. This study presents a hierarchical reinforcement learning method for decision making of self-driving cars, which does not depend on a large amount of labelled driving data. This method comprehensively considers both high-level manoeuvre selection and low-level motion control in both lateral and longitudinal directions. The authors firstly decompose the driving tasks into three manoeuvres, including driving in lane, right lane change and left lane change, and learn the sub-policy for each manoeuvre. Then, a master policy is learned to choose the manoeuvre policy to be executed in the current state. All policies, including master policy and manoeuvre policies, are represented by fully-connected neural networks and trained by using asynchronous parallel reinforcement learners, which builds a mapping from the sensory outputs to driving decisions. Different state spaces and reward functions are designed for each manoeuvre. They apply this method to a highway driving scenario, which demonstrates that it can realise smooth and safe decision making for self-driving cars.","tags":[],"title":"Hierarchical Reinforcement Learning for Self-Driving Decision-Making without Reliance on Labeled Driving Data","type":"publication"},{"authors":["Long Xin","Yiting Kong","Shengbo Eben Li","Jianyu Chen","Yang Guan","Masayoshi Tomizuka","Bingbing Nie"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"c7f34fc9dd0b5742d6db83ce0b55fc6e","permalink":"https://yangguan.me/publication/xinlong/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/xinlong/","section":"publication","summary":"Trajectory planning is of vital importance to decision-making for autonomous vehicles. Currently, there are three popular classes of cost-based trajectory planning methods: sampling-based, graph-search-based, and optimization-based. However, each of them has its own shortcomings, for example, high computational expense for sampling-based methods, low resolution for graph-search-based methods, and lack of global awareness for optimization-based methods. It leads to one of the challenges for trajectory planning for autonomous vehicles, which is improving planning efficiency while guaranteeing model feasibility. Therefore, this paper proposes a hybrid planning framework composed of two modules, which preserves the strength of both graph-search-based methods and optimization-based methods, thus enabling faster and smoother spatio-temporal trajectory planning in constrained dynamic environment. The proposed method first constructs spatio-temporal driving space based on directed acyclic graph and efficiently searches a spatio-temporal trajectory using the improved A* algorithm. Then taking the search result as reference, locally convex feasible driving area is designed and model predictive control is applied to further optimize the trajectory with a comprehensive consideration of vehicle kinematics and moving obstacles. Results simulated in four different scenarios all demonstrated feasible trajectories without emergency stop or abrupt steering change, which is kinematic-smooth to follow. Moreover, the average planning time was 31 ms, which only took 59.05%, 18.87%, and 0.69%, respectively, of that consumed by other state-of-the-art trajectory planning methods, namely, maximum interaction defensive policy, sampling-based method with iterative optimizations, and Graph-search-based method with Dynamic Programming.","tags":[],"title":"Enable faster and smoother spatio-temporal trajectory planning for autonomous vehicles in constrained dynamic environment","type":"publication"},{"authors":null,"categories":null,"content":"This project aims to develop a highly modularized and extensible RL library, with the ability of scaling to use hundreds of CPU cores for the high-throughput sampling, storing and updating. My works: 1) Summarised common procedures in different RL algorithms, according to that, abstracted the library by Worker, Learner, Buffer, Optimizer, Evaluator, Tester, and Trainer. Each has a certain functionality with clearly designed interface. 2) Proposed a general high-throughput and scalable learning architecture, which organizes arbitrary numbers of Workers, Learners, Buffers, and Evaluators in parallel, each with a CPU core, to enhance the sampling, learning, and replaying efficiency. 3) Developed the library by Tensorflow and Ray, which contains a cluster of state-of-the-art algorithm implementations, including MPG, DSAC, DDPG, ADP, TD3, SAC, PPO, TRPO.\n","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"9624328b9fbebf6034c86ec8db8ffad5","permalink":"https://yangguan.me/project/mpg/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/mpg/","section":"project","summary":"This project aims to develop a highly modularized and extensible RL library, with the ability of scaling to use hundreds of CPU cores for the high-throughput sampling, storing and updating. My works: 1) Summarised common procedures in different RL algorithms, according to that, abstracted the library by Worker, Learner, Buffer, Optimizer, Evaluator, Tester, and Trainer. Each has a certain functionality with clearly designed interface. 2) Proposed a general high-throughput and scalable learning architecture, which organizes arbitrary numbers of Workers, Learners, Buffers, and Evaluators in parallel, each with a CPU core, to enhance the sampling, learning, and replaying efficiency.","tags":["Reinforcement Learning"],"title":"Development of RL Library with High-throughput and Scalable Learning Architecture","type":"project"},{"authors":null,"categories":null,"content":"This project aims to build a general RL training and testing environment for the autonomous driving tasks. My works: 1) Combined a sophisticated vehicle dynamics and the large-scale traffic and map constructed using SUMO to realize co-simulation of the ego vehicle and its surrounding environment. 2) Designed the basic RL elements in the field of autonomous driving following the interface defined in Gym, such as the state, the action, the reward function, and the reset conditions etc, which can be easily reused or modified depending on your task. 3) Correspondingly, an analytic model of the environment is developed to facilitate its use in model-based RL algorithms. 4) A cluster of training and testing environments for our own use are also included, for instance, environments with specific designed RL elements (such as using bird view image as state, or using trajectory as action), ones with different simulation parameters (such as vehicle parameters, or the map), as well as environments for multiple automated vehicles (such as the centralized control environment, and the distributed control environment).\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"949d13db8c3491fc826ac622703bbf31","permalink":"https://yangguan.me/project/env_build/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/env_build/","section":"project","summary":"This project aims to build a general RL training and testing environment for the autonomous driving tasks. My works: 1) Combined a sophisticated vehicle dynamics and the large-scale traffic and map constructed using SUMO to realize co-simulation of the ego vehicle and its surrounding environment. 2) Designed the basic RL elements in the field of autonomous driving following the interface defined in Gym, such as the state, the action, the reward function, and the reset conditions etc, which can be easily reused or modified depending on your task.","tags":["Reinforcement Learning","Autonomous Driving"],"title":"Development of RL environment for large-scale autonomous driving tasks","type":"project"},{"authors":null,"categories":null,"content":"This project aims to develop a centralized coordination scheme of automated vehicles at an intersection without traffic signals using RL to address low computation efficiency suffered by current centralized coordination methods. My works: 1) Proposed model accelerated proximal policy optimization algorithm, which incorporates a prior model into PPO to facilitate sample efficiency. 2) Designed state with minimal length, and simplified reward function under consideration of safety, efficiency and task completion. The computing efficiency and traffic efficiency are respectively 400 times and 4.5 times higher that those of the baseline method based on MPC.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"38494d37a882d9cfc5d429e7ecb428f2","permalink":"https://yangguan.me/project/centralized/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/centralized/","section":"project","summary":"This project aims to develop a centralized coordination scheme of automated vehicles at an intersection without traffic signals using RL to address low computation efficiency suffered by current centralized coordination methods. My works: 1) Proposed model accelerated proximal policy optimization algorithm, which incorporates a prior model into PPO to facilitate sample efficiency. 2) Designed state with minimal length, and simplified reward function under consideration of safety, efficiency and task completion. The computing efficiency and traffic efficiency are respectively 400 times and 4.","tags":["Reinforcement Learning","Autonomous Driving"],"title":"Driving AI: Centralized Decision and Control for Multiple Vehicles at Crossroad by RL","type":"project"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$ renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ``` renders as\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ``` renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers. renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://yangguan.me/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Shengbo Eben Li","Yang Guan","Lian Hou","Hongbo Gao","Jingliang Duan","Shuang Liang","Yu Wang","Bo Cheng","Keqiang Li","Wei Ren","Jun Li"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"bbac5a510412a3da721371be93f2abfa","permalink":"https://yangguan.me/publication/av_survey/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/av_survey/","section":"publication","summary":"Autonomous driving is one of the three major innovations in automotive industry. Deep learning is a crucial method to improve automotive intelligence due to its outstanding abilities of data fitting, feature representation and model generalization. This paper reviews the technologies of deep neural network (DNN) for autonomous vehicles, which covers its history, main algorithms and key technical application. We first introduce the historical timeline of DNN, its “Unit-Layer-Network” architecture, and two representative models. We then summarize the training algorithms centered on back propagation (BP), labelled datasets and free-source frameworks for deep learning, followed by the introduction to computing platforms and model optimization technologies. Finally, we discuss the applications of DNN in autonomous vehicles, including object detection and semantic segmentation, hierarchical and end-to-end decision-making, longitudinal and lateral motion control, and point out the applicable methods and future works for different key problems of DNN in autonomous vehicles.","tags":[],"title":"Key technique of deep neural network and its applications in autonomous driving","type":"publication"},{"authors":["Yang Guan"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://yangguan.me/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yangguan.me/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Yang Guan","Shengbo Eben Li","Jingliang Duan","Wenjun Wang","Bo Cheng"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"00a8ff1249ae4ddab56bcbaee32d9f81","permalink":"https://yangguan.me/publication/mpd/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mpd/","section":"publication","summary":"Decision-making is one of the key technologies for self-driving cars. The high dependency of previously existing methods on human driving data or rules makes it difficult to model policies for different driving situations. In this research, a probabilistic decision-making method based on the Markov decision process (MDP) is proposed to deduce the optimal maneuver automatically in a two-lane highway scenario without using any human data. The decision-making issues in a traffic environment are formulated as the MDP by defining basic elements including states, actions and basic models. Transition and reward models are defined by using a complete prediction model of the surrounding cars. An optimal policy was deduced using a dynamic programing method and evaluated under a two-dimensional simulation environment. Results show that, at the given scenario, the self-driving car maintained safety and efficiency with the proposed policy. This paper presents a framework used to derive a driving policy for self-driving cars without relying on any human driving data or rules modeled by hand.","tags":[],"title":"Markov probabilistic decision making of self-driving cars in highway with random traffic flow: a simulation study","type":"publication"},{"authors":null,"categories":null,"content":"The project aims at designing an automatic decision making system for intelligent vehicle at a typical crossroad. My work focuses on decision making module design and implementation (by C++), mainly including behavior recognition of surrounding vehicles, decision of local driving target, and trajectory planning. Surrounding vehicles behavior recognition is achieved by a HMM model, which defines vehicle\u0026rsquo;s behavior as hidden state and infers probability distribution of behaviors every time step. Local driving target is decided by carefully designed rules, and its result is used for trajectory planning, which is realized by an algorithm based on DAG and A*.\n","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"333661a41d6536ad3c40e6fa68d30c19","permalink":"https://yangguan.me/project/planning/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/project/planning/","section":"project","summary":"The project aims at designing an automatic decision making system for intelligent vehicle at a typical crossroad. My work focuses on decision making module design and implementation (by C++), mainly including behavior recognition of surrounding vehicles, decision of local driving target, and trajectory planning. Surrounding vehicles behavior recognition is achieved by a HMM model, which defines vehicle\u0026rsquo;s behavior as hidden state and infers probability distribution of behaviors every time step. Local driving target is decided by carefully designed rules, and its result is used for trajectory planning, which is realized by an algorithm based on DAG and A*.","tags":["Autonomous Driving"],"title":"Automatic Decision Making and Planning at Crossroad","type":"project"},{"authors":["Yang Guan"],"categories":["Demo"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute       Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://yangguan.me/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"}]