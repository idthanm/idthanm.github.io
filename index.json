[{"authors":null,"categories":null,"content":"I am a Ph.D. candidate at Intelligent Driving Lab (iDLab), Tsinghua University. My research covers reinforcement learning, autonomous driving, and optimal control.\nCurrently, I am working on my Ph.D. in Tsinghua, where I was fortunate to be advised by Bo Cheng and Shengbo Eben Li. Since my graduate study, I have been committed to building more efficient and safer driving AI for the decision-making and control of automated vehicles, and to developing the next generation reinforcement learning algorithms that fuse both the data and the human knowledge for its application on advanced autonomous driving techniques. I also did several internships at Idriverplus, and TsingTech. I was born and grew up in Shanxi, China.\n","date":1614556800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1614556800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. candidate at Intelligent Driving Lab (iDLab), Tsinghua University. My research covers reinforcement learning, autonomous driving, and optimal control.\nCurrently, I am working on my Ph.D. in Tsinghua, where I was fortunate to be advised by Bo Cheng and Shengbo Eben Li.","tags":null,"title":"Yang Guan","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://idthanm.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Kaiming Tang","Shengbo Eben Li","Yuming Yin","Yang Guan","Jingliang Duan","Wenhan Cao","Jie Li"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"28edc765c7af387dfe13b07003c1a8b5","permalink":"https://idthanm.github.io/publication/rf/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/rf/","section":"publication","summary":"State estimation is critical to control systems, especially when the states cannot be directly measured. This paper presents an approximate optimal filter, which enables to use policy iteration technique to obtain the steady-state gain in linear Gaussian time-invariant systems. This design transforms the optimal filtering problem with minimum mean square error into an optimal control problem, called Approximate Optimal Filtering (AOF) problem. The equivalence holds given certain conditions about initial state distributions and policy formats, in which the system state is the estimation error, control input is the filter gain, and control objective function is the accumulated estimation error. We present a policy iteration algorithm to solve the AOF problem in steady-state. A classic vehicle state estimation problem finally evaluates the approximate filter. The results show that the policy converges to the steady-state Kalman gain, and its accuracy is within 2 %.","tags":[],"title":"Approximate Optimal Filter for Linear Gaussian Time-invariant Systems","type":"publication"},{"authors":["Yiting Kong","Yang Guan","Jingliang Duan","Shengbo Eben Li","Qi Sun","Bingbing Nie"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"5562fef4675674c463b9c3305f2e7324","permalink":"https://idthanm.github.io/publication/sdsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/sdsac/","section":"publication","summary":"Merging into the highway from the on-ramp is an essential scenario for automated driving. The decision-making under the scenario needs to balance the safety and efficiency performance to optimize a long-term objective, which is challenging due to the dynamic, stochastic, and adversarial characteristics. The Rule-based methods often lead to conservative driving on this task while the learning-based methods have difficulties meeting the safety requirements. In this paper, we propose an RL-based end-to-end decision-making method under a framework of offline training and online correction, called the Shielded Distributional Soft Actor-critic (SDSAC). The SDSAC adopts the policy evaluation with safety consideration and a safety shield parameterized with the barrier function in its offline training and online correction, respectively. These two measures support each other for better safety while not damaging the efficiency performance severely. We verify the SDSAC on an on-ramp merge scenario in simulation. The results show that the SDSAC has the best safety performance compared to baseline algorithms and achieves efficient driving simultaneously.","tags":[],"title":"Decision-Making under On-Ramp merge Scenarios by Distributional Soft Actor-Critic Algorithm","type":"publication"},{"authors":["Jingliang Duan","Yang Guan","Shengbo Eben Li","Yangang Ren","Bo Cheng"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"8fd00e154310c37b9fabd529509c9706","permalink":"https://idthanm.github.io/publication/dsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/dsac/","section":"publication","summary":"In current reinforcement learning (RL) methods, function approximation errors are known to lead to the overestimated or underestimated Q-value estimates, thus resulting in suboptimal policies. We show that the learning of a state-action return distribution function can be used to improve the Q-value estimation accuracy. We employ the return distribution function within the maximum entropy RL framework in order to develop what we call the Distributional Soft Actor-Critic (DSAC) algorithm, which is an off-policy method for continuous control setting. Unlike traditional distributional RL algorithms which typically only learn a discrete return distribution, DSAC directly learns a continuous return distribution by truncating the difference between the target and current distribution to prevent gradient explosion. Additionally, we propose a new Parallel Asynchronous Buffer-Actor-Learner architecture (PABAL) to improve the learning efficiency, which is a generalization of current high-throughput learning architectures. We evaluate our method on the suite of MuJoCo continuous control tasks, achieving state-of-the-art performance.","tags":[],"title":"Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors","type":"publication"},{"authors":["Yang Guan","Jingliang Duan","Shengbo Eben Li","Jie Li","Jianyu Chen","Bo Cheng"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"ec6e26e8fdd8936162c85ccc8e00e612","permalink":"https://idthanm.github.io/publication/mpg/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mpg/","section":"publication","summary":"Reinforcement learning (RL) has great potential in sequential decision-making. At present, the mainstream RL algorithms are data-driven, relying on millions of iterations and a large number of empirical data to learn a policy. Although data-driven RL may have excellent asymptotic performance, it usually yields slow convergence speed. As a comparison, model-driven RL employs a differentiable transition model to improve convergence speed, in which the policy gradient (PG) is calculated by using the backpropagation through time (BPTT) technique. However, such methods suffer from numerical instability, model error sensitivity and low computing efficiency, which may lead to poor policies. In this paper, a mixed policy gradient (MPG) method is proposed, which uses both empirical data and the transition model to construct the PG, so as to accelerate the convergence speed without losing the optimality guarantee. MPG contains two types of PG: 1) data-driven PG, which is obtained by directly calculating the derivative of the learned Q-value function with respect to actions, and 2) model-driven PG, which is calculated using BPTT based on the model-predictive return. We unify them by revealing the correlation between the upper bound of the unified PG error and the predictive horizon, where the data-driven PG is regraded as 0-step model-predictive return. Relying on that, MPG employs a rule-based method to adaptively adjust the weights of data-driven and model-driven PGs. In particular, to get a more accurate PG, the weight of the data-driven PG is designed to grow along the learning process while the other to decrease. Besides, an asynchronous learning framework is proposed to reduce the wall-clock time needed for each update iteration. Simulation results show that the MPG method achieves the best asymptotic performance and convergence speed compared with other baseline algorithms.","tags":[],"title":"Mixed Policy Gradient","type":"publication"},{"authors":["Baiyu Peng","Yao Mu","Jingliang Duan","Yang Guan","Shengbo Eben Li","Jianyu Chen"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"2c09672949bbf0cc6451ecced9c6fb20","permalink":"https://idthanm.github.io/publication/spil/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/spil/","section":"publication","summary":"Safety is essential for reinforcement learning (RL) applied in real-world tasks like autonomous driving. Chance constraints which guarantee the satisfaction of state constraints at a high probability are suitable to represent the requirements in real-world environment with uncertainty. Existing chance constrained RL methods like the penalty method and the Lagrangian method either exhibit periodic oscillations or cannot satisfy the constraints. In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. Taking a control perspective, we first interpret the penalty method and the Lagrangian method as proportional feedback and integral feedback control, respectively. Then, a proportional-integral Lagrangian method is proposed to steady learning process while improving safety. To prevent integral overshooting and reduce conservatism, we introduce the integral separation technique inspired by PID control. Finally, an analytical gradient of the chance constraint is utilized for model-based policy optimization. The effectiveness of SPIL is demonstrated by a narrow car-following task. Experiments indicate that compared with previous methods, SPIL improves the performance while guaranteeing safety, with a steady learning process.","tags":[],"title":"Separated Proportional-Integral Lagrangian for Chance Constrained Reinforcement Learning","type":"publication"},{"authors":["Yuhang Zhang","Yao Mu","Yujie Yang","Yang Guan","Shengbo Eben Li","Qi Sun","Jianyu Chen"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"2304010760d90a5f35ca2e0d4232853b","permalink":"https://idthanm.github.io/publication/lvm/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/lvm/","section":"publication","summary":"Reinforcement learning has shown great potential in developing high-level autonomous driving. However, for high-dimensional tasks, current RL methods suffer from low data efficiency and oscillation in the training process. This paper proposes an algorithm called Learn to drive with Virtual Memory (LVM) to overcome these problems. LVM compresses the high-dimensional information into compact latent states and learns a latent dynamic model to summarize the agent's experience. Various imagined latent trajectories are generated as virtual memory by the latent dynamic model. The policy is learned by propagating gradient through the learned latent model with the imagined latent trajectories and thus leads to high data efficiency. Furthermore, a double critic structure is designed to reduce the oscillation during the training process. The effectiveness of LVM is demonstrated by an image-input autonomous driving task, in which LVM outperforms the existing method in terms of data efficiency, learning stability, and control performance.","tags":[],"title":"Steadily Learn to Drive with Virtual Memory","type":"publication"},{"authors":["Yang Guan","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://idthanm.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Baiyu Peng","Yao Mu","Yang Guan","Shengbo Eben Li","Yuming Yin","Jianyu Chen"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"69094b0462aaf8bd11c412463b909033","permalink":"https://idthanm.github.io/publication/ccac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/ccac/","section":"publication","summary":"Safety constraints are essential for reinforcement learning (RL) applied in real-world situations. Chance constraints are suitable to represent the safety requirements in stochastic systems. Most existing RL methods with chance constraints have a low convergence rate, and only learn a conservative policy. In this paper, we propose a model-based chance constrained actor-critic (CCAC) algorithm which can efficiently learn a safe and non-conservative policy. Different from existing methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems, where the objective function and safe probability is simultaneously optimized with adaptive weights. In order to improve the convergence rate, CCAC utilizes the gradient of dynamic model to accelerate policy optimization. The effectiveness of CCAC is demonstrated by an aggressive car-following task. Experiments indicate that compared with previous methods, CCAC improves the performance by 57.6% while guaranteeing safety, with a five times faster convergence rate.","tags":[],"title":"Model-Based Actor-Critic with Chance Constraint for Stochastic System","type":"publication"},{"authors":["Yang Guan","Shengbo Eben Li","Jingliang Duan","Jie Li","Yangang Ren","Bo Cheng"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"3eb4d6fae1b99243ac7764f9627a19ab","permalink":"https://idthanm.github.io/publication/di_indi/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/di_indi/","section":"publication","summary":"Reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. In this paper, we classify RL into direct and indirect methods according to how they seek optimal policy of the Markov Decision Process (MDP) problem. The former solves optimal policy by directly maximizing an objective function using gradient descent method, in which the objective function is usually the expectation of accumulative future rewards. The latter indirectly finds the optimal policy by solving the Bellman equation, which is the sufficient and necessary condition from Bellman's principle of optimality. We take vanilla policy gradient and approximate policy iteration to study their internal relationship, and reveal that both direct and indirect methods can be unified in actor-critic architecture and are equivalent if we always choose stationary state distribution of current policy as initial state distribution of MDP. Finally, we classify the current mainstream RL algorithms and compare the differences between other criteria including value-based and policy-based, model-based and model-free.","tags":[],"title":"Direct and indirect reinforcement learning","type":"publication"},{"authors":null,"categories":null,"content":"The project aims to build an interpretable self-learning driving system by RL, for the real-time decision and control of automated vehicles. My works: 1) Formulated a general integrated decision and control framework, which utilizes RL as a way to solve constrained optimal control problems (OCP), and thus makes the output interpretable in the sense that it is the approximate solution of the OCP. The framework is promising to promote RL applications in real-world autonomous driving tasks. 2) Proposed a model-based RL algorithm for approximately solving large-scale constrained OCPs, where a generalized exterior point method is employed to find a feasible neural solution. 3) Carried out experiments both in simulation and in real world, yielding the best performance in terms of computing efficiency 10x and safety 100x compared with baseline methods.\n","date":1601164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601164800,"objectID":"27190523fd269ce0c7e4f86be54fa9b8","permalink":"https://idthanm.github.io/project/interpretable/","publishdate":"2020-09-27T00:00:00Z","relpermalink":"/project/interpretable/","section":"project","summary":"The project aims to build an interpretable self-learning driving system by RL, for the real-time decision and control of automated vehicles. My works: 1) Formulated a general integrated decision and control framework, which utilizes RL as a way to solve constrained optimal control problems (OCP), and thus makes the output interpretable in the sense that it is the approximate solution of the OCP.","tags":["Reinforcement Learning","Autonomous Driving"],"title":"Interpretable Driving AI with Highly Efficient Online Computation and Self-evolution Ability","type":"project"},{"authors":["Yang Guan","Yangang Ren","Shengbo Eben Li","Qi Sun","Laiquan Luo","Keqiang Li"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"f1ab6bbcb93421d96b5ed2a7f8c53e0f","permalink":"https://idthanm.github.io/publication/mappo/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mappo/","section":"publication","summary":"Connected vehicles will change the modes of future transportation management and organization, especially at an intersection without traffic light. Centralized coordination methods globally coordinate vehicles approaching the intersection from all sections by considering their states altogether. However, they need substantial computation resources since they own a centralized controller to optimize the trajectories for all approaching vehicles in real-time. In this paper, we propose a centralized coordination scheme of automated vehicles at an intersection without traffic signals using reinforcement learning (RL) to address low computation efficiency suffered by current centralized coordination methods. We first propose an RL training algorithm, model accelerated proximal policy optimization (MA-PPO), which incorporates a prior model into proximal policy optimization (PPO) algorithm to accelerate the learning process in terms of sample efficiency. Then we present the design of state, action and reward to formulate centralized coordination as an RL problem. Finally, we train a coordinate policy in a simulation setting and compare computing time and traffic efficiency with a coordination scheme based on model predictive control (MPC) method. Results show that our method spends only 1/400 of the computing time of MPC and increase the efficiency of the intersection by 4.5 times.","tags":[],"title":"Centralized Cooperation for Connected and Automated Vehicles at Intersections by Proximal Policy Optimization","type":"publication"},{"authors":["Yangang Ren","Jingliang Duan","Shengbo Eben Li","Yang Guan","Qi Sun"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"333d33df1352da1dd71d42baa5c197ed","permalink":"https://idthanm.github.io/publication/minmax_dsac/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/minmax_dsac/","section":"publication","summary":"Reinforcement learning (RL) has achieved remarkable performance in numerous sequential decision making and control tasks. However, a common problem is that learned nearly optimal policy always overfits to the training environment and may not be extended to situations never encountered during training. For practical applications, the randomness of environment usually leads to some devastating events, which should be the focus of safety-critical systems such as autonomous driving. In this paper, we introduce the minimax formulation and distributional framework to improve the generalization ability of RL algorithms and develop the Minimax Distributional Soft Actor-Critic (Minimax DSAC) algorithm. Minimax formulation aims to seek optimal policy considering the most severe variations from environment, in which the protagonist policy maximizes action-value function while the adversary policy tries to minimize it. Distributional framework aims to learn a state-action return distribution, from which we can model the risk of different returns explicitly, thereby formulating a risk-averse protagonist policy and a risk-seeking adversarial policy. We implement our method on the decision-making tasks of autonomous vehicles at intersections and test the trained policy in distinct environments. Results demonstrate that our method can greatly improve the generalization ability of the protagonist agent to different environmental variations.","tags":[],"title":"Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic","type":"publication"},{"authors":["Jie Li","Shengbo Eben Li","Yang Guan","Jingliang Duan","Wenyu Li","Yuming Yin"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"cfad4e91f6bc841ae4e9c83ef45a357c","permalink":"https://idthanm.github.io/publication/ternary/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/ternary/","section":"publication","summary":"The uncertainties in plant dynamics remain a challenge for nonlinear control problems. This paper develops a ternary policy iteration (TPI) algorithm for solving nonlinear robust control problems with bounded uncertainties. The controller and uncertainty of the system are considered as game players, and the robust control problem is formulated as a two-player zero-sum differential game. In order to solve the differential game, the corresponding Hamilton-Jacobi-Isaacs (HJI) equation is then derived. Three loss functions and three update phases are designed to match the identity equation, minimization and maximization of the HJI equation, respectively. These loss functions are defined by the expectation of the approximate Hamiltonian in a generated state set to prevent operating all the states in the entire state set concurrently. The parameters of value function and policies are directly updated by diminishing the designed loss functions using the gradient descent method. Moreover, zero-initialization can be applied to the parameters of the control policy. The effectiveness of the proposed TPI algorithm is demonstrated through two simulation studies. The simulation results show that the TPI algorithm can converge to the optimal solution for the linear plant, and has high resistance to disturbances for the nonlinear plant.","tags":[],"title":"Ternary Policy Iteration Algorithm for Nonlinear Robust Control","type":"publication"},{"authors":["Jingliang Duan","Shengbo Eben Li","Yang Guan","Qi Sun","Bo Cheng"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"bf982a6c02259b4cb686094a43683388","permalink":"https://idthanm.github.io/publication/hier/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/hier/","section":"publication","summary":"Decision making for self-driving cars is usually tackled by manually encoding rules from drivers’ behaviours or imitating drivers’ manipulation using supervised learning techniques. Both of them rely on mass driving data to cover all possible driving scenarios. This study presents a hierarchical reinforcement learning method for decision making of self-driving cars, which does not depend on a large amount of labelled driving data. This method comprehensively considers both high-level manoeuvre selection and low-level motion control in both lateral and longitudinal directions. The authors firstly decompose the driving tasks into three manoeuvres, including driving in lane, right lane change and left lane change, and learn the sub-policy for each manoeuvre. Then, a master policy is learned to choose the manoeuvre policy to be executed in the current state. All policies, including master policy and manoeuvre policies, are represented by fully-connected neural networks and trained by using asynchronous parallel reinforcement learners, which builds a mapping from the sensory outputs to driving decisions. Different state spaces and reward functions are designed for each manoeuvre. They apply this method to a highway driving scenario, which demonstrates that it can realise smooth and safe decision making for self-driving cars.","tags":[],"title":"Hierarchical Reinforcement Learning for Self-Driving Decision-Making without Reliance on Labeled Driving Data","type":"publication"},{"authors":["Long Xin","Yiting Kong","Shengbo Eben Li","Jianyu Chen","Yang Guan","Masayoshi Tomizuka","Bingbing Nie"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"c7f34fc9dd0b5742d6db83ce0b55fc6e","permalink":"https://idthanm.github.io/publication/xinlong/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/xinlong/","section":"publication","summary":"Trajectory planning is of vital importance to decision-making for autonomous vehicles. Currently, there are three popular classes of cost-based trajectory planning methods: sampling-based, graph-search-based, and optimization-based. However, each of them has its own shortcomings, for example, high computational expense for sampling-based methods, low resolution for graph-search-based methods, and lack of global awareness for optimization-based methods. It leads to one of the challenges for trajectory planning for autonomous vehicles, which is improving planning efficiency while guaranteeing model feasibility. Therefore, this paper proposes a hybrid planning framework composed of two modules, which preserves the strength of both graph-search-based methods and optimization-based methods, thus enabling faster and smoother spatio-temporal trajectory planning in constrained dynamic environment. The proposed method first constructs spatio-temporal driving space based on directed acyclic graph and efficiently searches a spatio-temporal trajectory using the improved A* algorithm. Then taking the search result as reference, locally convex feasible driving area is designed and model predictive control is applied to further optimize the trajectory with a comprehensive consideration of vehicle kinematics and moving obstacles. Results simulated in four different scenarios all demonstrated feasible trajectories without emergency stop or abrupt steering change, which is kinematic-smooth to follow. Moreover, the average planning time was 31 ms, which only took 59.05%, 18.87%, and 0.69%, respectively, of that consumed by other state-of-the-art trajectory planning methods, namely, maximum interaction defensive policy, sampling-based method with iterative optimizations, and Graph-search-based method with Dynamic Programming.","tags":[],"title":"Enable faster and smoother spatio-temporal trajectory planning for autonomous vehicles in constrained dynamic environment","type":"publication"},{"authors":null,"categories":null,"content":"This project aims to develop a highly modularized and extensible RL library, with the ability of scaling to use hundreds of CPU cores for the high-throughput sampling, storing and updating. My works: 1) Summarised common procedures in different RL algorithms, according to that, abstracted the library by Worker, Learner, Buffer, Optimizer, Evaluator, Tester, and Trainer. Each has a certain functionality with clearly designed interface. 2) Proposed a general high-throughput and scalable learning architecture, which organizes arbitrary numbers of Workers, Learners, Buffers, and Evaluators in parallel, each with a CPU core, to enhance the sampling, learning, and replaying efficiency. 3) Developed the library by Tensorflow and Ray, which contains a cluster of state-of-the-art algorithm implementations, including MPG, DSAC, DDPG, ADP, TD3, SAC, PPO, TRPO.\n","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"9624328b9fbebf6034c86ec8db8ffad5","permalink":"https://idthanm.github.io/project/mpg/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/mpg/","section":"project","summary":"This project aims to develop a highly modularized and extensible RL library, with the ability of scaling to use hundreds of CPU cores for the high-throughput sampling, storing and updating. My works: 1) Summarised common procedures in different RL algorithms, according to that, abstracted the library by Worker, Learner, Buffer, Optimizer, Evaluator, Tester, and Trainer.","tags":["Reinforcement Learning"],"title":"Development of RL Library with High-throughput and Scalable Learning Architecture","type":"project"},{"authors":null,"categories":null,"content":"This project aims to develop a centralized coordination scheme of automated vehicles at an intersection without traffic signals using RL to address low computation efficiency suffered by current centralized coordination methods. My works: 1) Proposed model accelerated proximal policy optimization algorithm, which incorporates a prior model into PPO to facilitate sample efficiency. 2) Designed state with minimal length, and simplified reward function under consideration of safety, efficiency and task completion. The computing efficiency and traffic efficiency are respectively 400 times and 4.5 times higher that those of the baseline method based on MPC.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"38494d37a882d9cfc5d429e7ecb428f2","permalink":"https://idthanm.github.io/project/centralized/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/centralized/","section":"project","summary":"This project aims to develop a centralized coordination scheme of automated vehicles at an intersection without traffic signals using RL to address low computation efficiency suffered by current centralized coordination methods.","tags":["Reinforcement Learning","Autonomous Driving"],"title":"Driving AI: Centralized Decision and Control for Multiple Vehicles at Crossroad by RL","type":"project"},{"authors":["Shengbo Eben Li","Yang Guan","Lian Hou","Hongbo Gao","Jingliang Duan","Shuang Liang","Yu Wang","Bo Cheng","Keqiang Li","Wei Ren","Jun Li"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"bbac5a510412a3da721371be93f2abfa","permalink":"https://idthanm.github.io/publication/av_survey/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/av_survey/","section":"publication","summary":"Autonomous driving is one of the three major innovations in automotive industry. Deep learning is a crucial method to improve automotive intelligence due to its outstanding abilities of data fitting, feature representation and model generalization. This paper reviews the technologies of deep neural network (DNN) for autonomous vehicles, which covers its history, main algorithms and key technical application. We first introduce the historical timeline of DNN, its “Unit-Layer-Network” architecture, and two representative models. We then summarize the training algorithms centered on back propagation (BP), labelled datasets and free-source frameworks for deep learning, followed by the introduction to computing platforms and model optimization technologies. Finally, we discuss the applications of DNN in autonomous vehicles, including object detection and semantic segmentation, hierarchical and end-to-end decision-making, longitudinal and lateral motion control, and point out the applicable methods and future works for different key problems of DNN in autonomous vehicles.","tags":[],"title":"Key technique of deep neural network and its applications in autonomous driving","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://idthanm.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Yang Guan","Shengbo Eben Li","Jingliang Duan","Wenjun Wang","Bo Cheng"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"00a8ff1249ae4ddab56bcbaee32d9f81","permalink":"https://idthanm.github.io/publication/mpd/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/mpd/","section":"publication","summary":"Decision-making is one of the key technologies for self-driving cars. The high dependency of previously existing methods on human driving data or rules makes it difficult to model policies for different driving situations. In this research, a probabilistic decision-making method based on the Markov decision process (MDP) is proposed to deduce the optimal maneuver automatically in a two-lane highway scenario without using any human data. The decision-making issues in a traffic environment are formulated as the MDP by defining basic elements including states, actions and basic models. Transition and reward models are defined by using a complete prediction model of the surrounding cars. An optimal policy was deduced using a dynamic programing method and evaluated under a two-dimensional simulation environment. Results show that, at the given scenario, the self-driving car maintained safety and efficiency with the proposed policy. This paper presents a framework used to derive a driving policy for self-driving cars without relying on any human driving data or rules modeled by hand.","tags":[],"title":"Markov probabilistic decision making of self-driving cars in highway with random traffic flow: a simulation study","type":"publication"},{"authors":null,"categories":null,"content":"The project aims at designing an automatic decision making system for intelligent vehicle at a typical crossroad. My work focuses on decision making module design and implementation (by C++), mainly including behavior recognition of surrounding vehicles, decision of local driving target, and trajectory planning. Surrounding vehicles behavior recognition is achieved by a HMM model, which defines vehicle\u0026rsquo;s behavior as hidden state and infers probability distribution of behaviors every time step. Local driving target is decided by carefully designed rules, and its result is used for trajectory planning, which is realized by an algorithm based on DAG and A*.\n","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"333661a41d6536ad3c40e6fa68d30c19","permalink":"https://idthanm.github.io/project/planning/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/project/planning/","section":"project","summary":"The project aims at designing an automatic decision making system for intelligent vehicle at a typical crossroad. My work focuses on decision making module design and implementation (by C++), mainly including behavior recognition of surrounding vehicles, decision of local driving target, and trajectory planning.","tags":["Autonomous Driving"],"title":"Automatic Decision Making and Planning at Crossroad","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://idthanm.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]